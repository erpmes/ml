import torchimport torch.nn as nn### Define Modelclass BasicBlock(nn.Module):    def __init__(self, input_dim, output_dim):        super(BasicBlock, self).__init__()        self.block = nn.Sequential(            nn.Linear(input_dim, output_dim),            nn.ReLU(),        )    def forward(self, x):        x = self.block(x)        return xclass Classifier(nn.Module):    def __init__(self, input_dim, output_dim=41, hidden_layers=1, hidden_dim=256):        super(Classifier, self).__init__()        self.fc = nn.Sequential(            BasicBlock(input_dim, hidden_dim),            *[BasicBlock(hidden_dim, hidden_dim) for _ in range(hidden_layers)],            nn.Linear(hidden_dim, output_dim)        )    def forward(self, x):        x = self.fc(x)        return xconcat_nframes = 11# model parametersinput_dim = 39 * concat_nframes # the input dim of the model, you should not change the valuehidden_layers = 1               # the number of hidden layershidden_dim = 256                # the hidden dimdevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'print(f'DEVICE: {device}')# load modelmodel = Classifier(input_dim=input_dim, hidden_layers=hidden_layers, hidden_dim=hidden_dim).to(device)model.load_state_dict(torch.load('./model.ckpt'))# 提取模型权重weights = {}for name, param in model.named_parameters():    weights[name] = param.data.numpy()# 保存权重为txt文件for name, value in weights.items():    with open(f'./{name}.txt', 'w') as f:        f.write('\n'.join(value.flatten().astype(str)))